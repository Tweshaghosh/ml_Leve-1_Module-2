{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('3.9')"
  },
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from selenium.webdriver.remote.utils import dump_json\n",
    "\n",
    "\n",
    "class ScrapeThePage:\n",
    "    def __init__(self):\n",
    "        path = \"/Users/bhaskarghosh/BGEverything/Twesha/stemaway/ml_Leve-1_Module-2\"\n",
    "        print(path)\n",
    "        webDriverWithPath = path + \"/chromedriver\"\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\n",
    "            \"--ignore-certificate-errors\"\n",
    "        )  # Ignore security certificates\n",
    "        chrome_options.add_argument(\"--incognito\")  # Use Chrome in Incognito mode\n",
    "        chrome_options.add_argument(\"--headless\")  # Run in background\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        self.driver = webdriver.Chrome(\n",
    "            executable_path=webDriverWithPath,\n",
    "            options=chrome_options,\n",
    "        )\n",
    "#         chrome_options = webdriver.ChromeOptions()\n",
    "#         chrome_options.add_argument('--headless')\n",
    "#         chrome_options.add_argument('--no-sandbox')\n",
    "#         chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "# # open it, go to a website, and get results\n",
    "#         self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        now = datetime.now()\n",
    "        date_time = now.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "        self.result_file_name = path + \"/sa_l2m2_\" + date_time + \".json\"\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def load_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        # Load the entire webage by scrolling to the bottom\n",
    "        \n",
    "        lastHeight = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            # Scroll to bottom of page\n",
    "            self.driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "            )\n",
    "\n",
    "            # Wait for new page segment to load\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            newHeight = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if newHeight == lastHeight:\n",
    "                break\n",
    "            lastHeight = newHeight\n",
    "        page_source = self.driver.page_source\n",
    "        return page_source\n",
    "\n",
    "    def getCommunities(self, url):\n",
    "        dict_comm = {}\n",
    "        page_source = self.load_page(url)\n",
    "        l1_soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        all_headers = l1_soup.find_all(\"h2\")\n",
    "        for h in all_headers:\n",
    "            hdr = h.text\n",
    "            hdr = hdr.strip(\" –\")\n",
    "            dict_comm[hdr] = {}\n",
    "            centries = (h.findNext(\"div\")).find_all(\"div\", \"content-item community\")\n",
    "            for ce in centries:\n",
    "                name = (ce.find(\"h3\")).text\n",
    "                dict_comm[hdr][name] = {}\n",
    "                footer = ce.find_all(\"footer\")\n",
    "                fentries = footer[0].find_all(\"div\")\n",
    "                mem = fentries[0].span.text\n",
    "                act = fentries[1].span.text\n",
    "                lang = fentries[2].span.text\n",
    "                link = (fentries[3].find(\"a\")).attrs[\"href\"]\n",
    "                dict_comm[hdr][name][\"members\"] = mem\n",
    "                dict_comm[hdr][name][\"activity\"] = act\n",
    "                dict_comm[hdr][name][\"language\"] = lang\n",
    "                dict_comm[hdr][name][\"link\"] = link\n",
    "        return dict_comm\n",
    "\n",
    "    def store_result(self):\n",
    "      with open(self.result_file_name, \"w+\") as fp:\n",
    "        json.dump(\n",
    "          self.comm_dict[\"Automobiles\"],\n",
    "          fp,\n",
    "          skipkeys=False,\n",
    "          ensure_ascii=True,\n",
    "          check_circular=True,\n",
    "          allow_nan=True,\n",
    "          cls=None,\n",
    "          indent=4,\n",
    "          separators=None,\n",
    "          default=None,\n",
    "          sort_keys=False,\n",
    "        )\n",
    "\n",
    "    def getCategory(self, cattaglist):\n",
    "        c = {}\n",
    "        c[\"category\"] = \"\"\n",
    "        c[\"tags\"] = []\n",
    "        if cattaglist is None:\n",
    "            return c\n",
    "        for v in cattaglist:\n",
    "            if \"category\" in v:\n",
    "                c[\"category\"] = v.split(\"category-\")[-1]\n",
    "            if \"tag\" in v:\n",
    "                c[\"tags\"].append(v.split(\"tag-\")[-1])\n",
    "        return c\n",
    "\n",
    "    # Build out the entire attribute map\n",
    "    # Topic Title\tCategory\tTags\tLeading Post\tPost Replies\tCreated_at\tReplies\n",
    "    def getTopics(self, url):\n",
    "        all_topics = []\n",
    "        print(url)\n",
    "        page_source = self.load_page(url)\n",
    "        l2_soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        all_headers = l2_soup.find_all(\"tr\")\n",
    "        count = 1\n",
    "        for h in all_headers:\n",
    "            if count > 35:\n",
    "              break\n",
    "            if count % 20 == 0:\n",
    "              self.comm_dict[\"Automobiles\"][\"Car Talk Community\"][\"topics\"] = all_topics\n",
    "              self.store_result()\n",
    "            count += 1\n",
    "            print(count)\n",
    "            # Get Category and tags†\n",
    "            c = self.getCategory(h.get(\"class\"))\n",
    "            topicObj = {}\n",
    "            topicObj[\"name\"] = \"\"\n",
    "            topicObj[\"category\"] = c[\"category\"]\n",
    "            topicObj[\"tags\"] = c[\"tags\"]\n",
    "            # get the rest like Title Post replies created\n",
    "            all_d = h.find_all(\"td\")\n",
    "            for d in all_d:\n",
    "                if \"main-link\" in d[\"class\"]:\n",
    "                    title = d.find(class_=\"link-top-line\")\n",
    "                    topicObj[\"name\"] = title.text.strip()\n",
    "                    topicObj[\"link\"] = url + d.find(\"a\").attrs[\"href\"]\n",
    "                if \"posts\" in d[\"class\"]:\n",
    "                    num_posts = d.find(class_=\"number\")\n",
    "                    topicObj[\"num_posts\"] = num_posts.text\n",
    "                if \"views\" in d[\"class\"]:\n",
    "                    num_views = d.find(class_=\"number\")\n",
    "                    nvt = num_views.text.strip()\n",
    "                    if \"k\" in list(nvt):\n",
    "                        nv = nvt.strip(\"k\")\n",
    "                        nvt = str(int(float(nv) * 1000))\n",
    "                    topicObj[\"num_views\"] = nvt\n",
    "                if \"age\" in d[\"class\"]:\n",
    "                    post_age = d[\"title\"].strip().split(\"\\n\")\n",
    "                    topicObj[\"first_post\"] = post_age[0].split(\": \")[-1]\n",
    "                    topicObj[\"last_post\"] = post_age[1].split(\": \")[-1]\n",
    "                    datetime_object_f = datetime.strptime(\n",
    "                        topicObj[\"first_post\"], \"%b %d, %Y %I:%M %p\"\n",
    "                    )\n",
    "                    datetime_object_l = datetime.strptime(\n",
    "                        topicObj[\"last_post\"], \"%b %d, %Y %I:%M %p\"\n",
    "                    )\n",
    "                    diff = datetime.now() - datetime_object_f\n",
    "                    topicObj[\"first_post_age\"] = diff.days\n",
    "                    diff = datetime.now() - datetime_object_l\n",
    "                    topicObj[\"last_post_age\"] = diff.days\n",
    "            if topicObj[\"name\"]:\n",
    "                if topicObj[\"link\"]:\n",
    "                    topicObj[\"sentences\"] = self.getPosts(topicObj[\"link\"])\n",
    "                all_topics.append(topicObj)\n",
    "        return all_topics\n",
    "\n",
    "    def getPosts(self, url):\n",
    "        clean = re.compile(\"<.*?>\")\n",
    "        sentence_list = []\n",
    "        count_words = Counter()\n",
    "\n",
    "        page_source = self.load_page(url)\n",
    "        l3_soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        all_entries = l3_soup.find_all(class_=\"cooked\")\n",
    "        print(url)\n",
    "        for e in all_entries:\n",
    "            part = e.find(\"p\")\n",
    "            if part:\n",
    "                sntnc = part.get_text()\n",
    "                sntnc = sntnc.lower()\n",
    "                sntnc = re.sub(clean, \"\", sntnc)\n",
    "                wrdl = sntnc.split()\n",
    "                new_l = []\n",
    "                for w in wrdl:\n",
    "                    if w.isascii():\n",
    "                        new_l.append(w)\n",
    "                sntnc = \" \".join(new_l)\n",
    "                sntnc = self.clean_text(sntnc)\n",
    "                set_sntnc = set(sentence_list)\n",
    "                if sntnc not in set_sntnc:\n",
    "                    sentence_list.append(sntnc)\n",
    "        return sentence_list\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        REPLACE_BY_SPACE_RE = re.compile(\"[/(){}\\[\\]'\\“\\”\\’\\|@,;]\")\n",
    "        BAD_SYMBOLS_RE = re.compile(\"[^0-9a-z #+_]\")\n",
    "        STOPWORDS = set(stopwords.words(\"english\"))\n",
    "        NUM_RE = re.compile(\" \\d+\")\n",
    "\n",
    "        text = REPLACE_BY_SPACE_RE.sub(\n",
    "            \" \", text\n",
    "        )  # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "        text = BAD_SYMBOLS_RE.sub(\n",
    "            \"\", text\n",
    "        )  # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "        text = \" \".join(\n",
    "            word for word in text.split() if word not in STOPWORDS\n",
    "        )  # delete stopwors from text\n",
    "        text = NUM_RE.sub(\"\", text)  # replace numbers with nothing in text\n",
    "        return text\n",
    "\n",
    "    def runApp(self, url):\n",
    "        self.comm_dict = self.getCommunities(url)\n",
    "        \"\"\"\n",
    "            Only looking at one area.\n",
    "        \"\"\"\n",
    "\n",
    "        self.all_topics = self.getTopics(\n",
    "            self.comm_dict[\"Automobiles\"][\"Car Talk Community\"][\"link\"]\n",
    "        )\n",
    "        self.comm_dict[\"Automobiles\"][\"Car Talk Community\"][\"topics\"] = self.all_topics\n",
    "        self.store_result()\n",
    "\n",
    "\n",
    "# url = \"https://forums.tapas.io/t/post-the-last-sentence-you-wrote/29878\"\n",
    "url = \"https://www.discoursehub.com/communities/\"\n",
    "stp = ScrapeThePage()\n",
    "stp.runApp(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1558\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open (\"/Users/bhaskarghosh/BGEverything/Twesha/stemaway/ml_Leve-1_Module-2/sa_l2m2_1800.json\", \"r+\") as fp:\n",
    "    data = json.load(fp)\n",
    "    print(len(data[\"Car Talk Community\"][\"topics\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import logging\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "class mlModels:\n",
    "    def __init__(self, fname):\n",
    "        with open(fname, \"r+\") as fp:\n",
    "            self.data = json.load(fp)\n",
    "\n",
    "    def createDF(self):\n",
    "        # Convert it into Panda Dataframe\n",
    "        df_dict = {}\n",
    "        for topic in self.data[\"Car Talk Community\"][\"topics\"]:\n",
    "            for k, v in topic.items():\n",
    "                # if \"sentences\" in k:\n",
    "                #     continue\n",
    "                if k not in df_dict:\n",
    "                    df_dict[k] = []\n",
    "                df_dict[k].append(v)\n",
    "        self.df = pd.DataFrame(df_dict)\n",
    "\n",
    "    def cleanData(self):\n",
    "        return\n",
    "\n",
    "    def word_count(self):\n",
    "        self.df[\"word_count\"] = self.df[\"sentences\"].apply(\n",
    "            lambda x: len((\" \".join(x)).split(\" \"))\n",
    "        )\n",
    "        self.df[[\"name\", \"word_count\"]].head()\n",
    "\n",
    "    def stop_words(self):\n",
    "        stop = stopwords.words(\"english\")\n",
    "        self.df[\"stop_words\"] = self.df[\"sentences\"].apply(\n",
    "            lambda x: len([j for j in ((\" \".join(x)).split(\" \")) if j in stop])\n",
    "        )\n",
    "        self.df[[\"name\", \"stop_words\"]].head()\n",
    "\n",
    "    def plot_word_cloud(self, text):\n",
    "        wordcloud_instance = WordCloud(\n",
    "            width=800,\n",
    "            height=800,\n",
    "            background_color=\"black\",\n",
    "            stopwords=None,\n",
    "            min_font_size=10,\n",
    "        ).generate(text)\n",
    "\n",
    "        plt.figure(figsize=(8, 8), facecolor=None)\n",
    "        plt.imshow(wordcloud_instance)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_word_cloud_for_category(self, category):\n",
    "        text_df = self.df.loc[self.df[\"category\"] == str(category)]\n",
    "        texts = \"\"\n",
    "        for index, item in text_df.iterrows():\n",
    "            texts = texts + \" \" + \" \".join(item[\"sentences\"])\n",
    "        self.plot_word_cloud(texts)\n",
    "        \n",
    "fname = \"./sa_l2m2_07_08_2021_14_37_45.json\"\n",
    "mlm = mlModels(fname)\n",
    "mlm.createDF()\n",
    "mlm.word_count()\n",
    "set_cat = set(mlm.df[\"category\"])\n",
    "for c in list(set_cat):\n",
    "    print(c)\n",
    "    mlm.plot_word_cloud_for_category(c)\n",
    "\n",
    "# mlm.plot_word_cloud_for_category(\"general-discussion\")\n",
    "# mlm.plot_word_cloud_for_category(\"repair-and-maintenance\")\n",
    "# mlm.plot_word_cloud_for_category(\"aswoo\")\n",
    "#print(mlm.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}